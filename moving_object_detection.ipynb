{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path and model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path  = os.path.join(os.getcwd(), 'src','classroom.mp4')\n",
    "\n",
    "## checkpoints for sam\n",
    "sam_checkpoints = \"checkpoints\"\n",
    "vit_h = \"sam_vit_h_4b8939.pth\"\n",
    "vit_b = \"sam_vit_b_01ec64.pth\"\n",
    "vit_l = \"sam_vit_l_0b3195.pth\"\n",
    "\n",
    "## check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## yolo model\n",
    "yolo_model = YOLO('yolov8n.pt').to(device)\n",
    "\n",
    "## sam model\n",
    "model_type = \"vit_l\"\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(sam_checkpoints, vit_l))\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \n",
    "    results = yolo_model(frame, conf=0.25, classes=[0])\n",
    "    \n",
    "    ## Process results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        \n",
    "    bbox = boxes.xyxy\n",
    "    print('bbox shape: ', bbox.shape)\n",
    "    #confidences = boxes.conf\n",
    "    #classes = boxes.cls \n",
    "    #predictor = SamPredictor(sam)\n",
    "    predictor.set_image(frame)\n",
    "    \n",
    "    input_boxes = bbox.to(predictor.device)\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, frame.shape[:2])\n",
    "    \n",
    "    masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    "    )\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_mask2img(mask):\n",
    "    palette = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (255, 0, 0),\n",
    "        2: (0, 255, 0),\n",
    "        3: (0, 0, 255),\n",
    "        4: (0, 255, 255),\n",
    "    }\n",
    "    items = mask.shape[0]\n",
    "    rows = mask.shape[1]\n",
    "    cols = mask.shape[2]\n",
    "    image = np.zeros((items, rows, cols, 3), dtype=np.uint8)\n",
    "    image[:, :, :, 0] = mask * palette[1][0]\n",
    "    image[:, :, :, 1] = mask * palette[1][1]\n",
    "    image[:, :, :, 2] = mask * palette[1][2]\n",
    "    return image\n",
    "\n",
    "def optimized_show_mask(masks):\n",
    "    masks = np.squeeze(masks, axis = 1)\n",
    "    separate_rgb_masks = optimized_mask2img(masks)\n",
    "    combined_mask = np.sum(separate_rgb_masks, axis = 0)\n",
    "    return combined_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in loading the video\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error in loading the video\")\n",
    "    \n",
    "i = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    start = time.time()\n",
    "    masks = process_frame(frame)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"1. Time taken for frame {} is {}\".format(i, end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    colour_mask = optimized_show_mask(masks)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"2. Time taken for frame {} is {}\".format(i, end-start))\n",
    "    # frame = frame + colour_mask*0.3\n",
    "    \n",
    "    #dispaly frame and colour mask in same window\n",
    "    start = time.time()\n",
    "    frame = ((frame/np.max(frame))*255).astype(np.uint8)\n",
    "    colour_mask = cv2.addWeighted(colour_mask.astype(np.uint8), 0.3, frame, 0.7, 0, colour_mask.astype(np.uint8))\n",
    "    \n",
    "    \n",
    "    #-----------for contours -------\n",
    "    masks = np.squeeze(masks.detach().cpu().numpy(), axis = 1).astype(np.uint8)\n",
    "    print('masks shape: ', masks.shape, masks.shape[0], np.unique(masks))\n",
    "    for dim in range(masks.shape[0]):\n",
    "        print('in shape: ', masks[dim, :, :].shape)\n",
    "        contours, hierarchy = cv2.findContours(image = masks[dim, :, :], mode = cv2.RETR_TREE, method = cv2.CHAIN_APPROX_NONE)\n",
    "        cv2.drawContours(image = frame, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=1, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('frame', colour_mask)\n",
    "    end = time.time()\n",
    "    print(\"3. Time taken for frame {} is {}\".format(i, end-start))\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
