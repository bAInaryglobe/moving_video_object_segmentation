{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path and model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path  = os.path.join(os.getcwd(), 'src','classroom.mp4')\n",
    "\n",
    "## checkpoints for sam\n",
    "sam_checkpoints = \"checkpoints\"\n",
    "vit_h = \"sam_vit_h_4b8939.pth\"\n",
    "vit_b = \"sam_vit_b_01ec64.pth\"\n",
    "vit_l = \"sam_vit_l_0b3195.pth\"\n",
    "\n",
    "## check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## yolo model\n",
    "yolo_model = YOLO('yolov8n.pt').to(device)\n",
    "\n",
    "## sam model\n",
    "model_type = \"vit_l\"\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(sam_checkpoints, vit_l))\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \n",
    "    results = yolo_model(frame, conf=0.25, classes=[0])\n",
    "    \n",
    "    ## Process results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        \n",
    "    bbox = boxes.xyxy\n",
    "    #confidences = boxes.conf\n",
    "    #classes = boxes.cls \n",
    "    #predictor = SamPredictor(sam)\n",
    "    predictor.set_image(frame)\n",
    "    \n",
    "    input_boxes = bbox.to(predictor.device)\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, frame.shape[:2])\n",
    "    \n",
    "    masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    "    )\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask2img(mask):\n",
    "    palette = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (255, 0, 0),\n",
    "        2: (0, 255, 0),\n",
    "        3: (0, 0, 255),\n",
    "        4: (0, 255, 255),\n",
    "    }\n",
    "    \n",
    "    palette_tensor = torch.tensor([palette[x] for x in mask.flatten()], dtype=torch.uint8)\n",
    "    image = palette_tensor.reshape(mask.shape[0], mask.shape[1], 3)\n",
    "    return image\n",
    "\n",
    "def show_mask(masks, random_color=False):\n",
    "    mask_images = [mask2img(torch.squeeze(mask).cpu().numpy()) for mask in masks]\n",
    "    combined_mask = torch.sum(torch.stack(mask_images, dim=0), dim=0)\n",
    "    return combined_mask.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 23.0ms\n",
      "Speed: 3.0ms preprocess, 23.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Time taken for frame 0 is 0.9768147468566895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 23.0ms\n",
      "Speed: 2.0ms preprocess, 23.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 0 is 3.8341541290283203\n",
      "3. Time taken for frame 0 is 0.02499842643737793\n",
      "1. Time taken for frame 1 is 0.9142334461212158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 1 is 3.3505029678344727\n",
      "3. Time taken for frame 1 is 0.010968923568725586\n",
      "1. Time taken for frame 2 is 0.863013505935669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 33.0ms\n",
      "Speed: 2.0ms preprocess, 33.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 2 is 3.3431832790374756\n",
      "3. Time taken for frame 2 is 0.010030031204223633\n",
      "1. Time taken for frame 3 is 0.9471521377563477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 3 is 3.842090368270874\n",
      "3. Time taken for frame 3 is 0.011001348495483398\n",
      "1. Time taken for frame 4 is 0.8942739963531494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 4 is 3.3581533432006836\n",
      "3. Time taken for frame 4 is 0.009998559951782227\n",
      "1. Time taken for frame 5 is 0.9173538684844971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 5 is 3.4241602420806885\n",
      "3. Time taken for frame 5 is 0.010000228881835938\n",
      "1. Time taken for frame 6 is 0.8531053066253662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 6 is 3.3597705364227295\n",
      "3. Time taken for frame 6 is 0.00999903678894043\n",
      "1. Time taken for frame 7 is 0.9042179584503174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 7 is 3.3455116748809814\n",
      "3. Time taken for frame 7 is 0.011001825332641602\n",
      "1. Time taken for frame 8 is 0.8813011646270752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 8 is 3.3629238605499268\n",
      "3. Time taken for frame 8 is 0.01100301742553711\n",
      "1. Time taken for frame 9 is 0.9050312042236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 29.0ms\n",
      "Speed: 3.0ms preprocess, 29.0ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 9 is 3.814703941345215\n",
      "3. Time taken for frame 9 is 0.008994340896606445\n",
      "1. Time taken for frame 10 is 0.8716320991516113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 10 is 3.8407680988311768\n",
      "3. Time taken for frame 10 is 0.009029626846313477\n",
      "1. Time taken for frame 11 is 0.9148571491241455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 20.0ms\n",
      "Speed: 1.0ms preprocess, 20.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Time taken for frame 11 is 3.471442699432373\n",
      "3. Time taken for frame 11 is 0.01099848747253418\n",
      "1. Time taken for frame 12 is 0.8535547256469727\n",
      "2. Time taken for frame 12 is 3.909257173538208\n",
      "3. Time taken for frame 12 is 0.00899958610534668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error in loading the video\")\n",
    "    \n",
    "i = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    start = time.time()\n",
    "    masks = process_frame(frame)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"1. Time taken for frame {} is {}\".format(i, end-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    colour_mask = show_mask(masks)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"2. Time taken for frame {} is {}\".format(i, end-start))\n",
    "    # frame = frame + colour_mask*0.3\n",
    "    \n",
    "    #dispaly frame and colour mask in same window\n",
    "    start = time.time()\n",
    "    frame = ((frame/np.max(frame))*255).astype(np.uint8)\n",
    "    colour_mask = cv2.addWeighted(colour_mask.astype(np.uint8), 0.3, frame, 0.7, 0, colour_mask.astype(np.uint8))\n",
    "    #cv2.imshow('frame', frame)\n",
    "    cv2.imshow('frame', colour_mask)\n",
    "    end = time.time()\n",
    "    print(\"3. Time taken for frame {} is {}\".format(i, end-start))\n",
    "    \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objectdetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
